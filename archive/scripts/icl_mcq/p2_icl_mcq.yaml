# Sample configuration for HuggingFace model experiment
# This file defines the basic parameters for running an evaluation experiment

# Model to use for the experiment (any HuggingFace model name or local path)
# model_name: "meta-llama/Llama-3.2-1B-Instruct"
model_name: "meta-llama/Llama-3.1-8B-Instruct"


# Path where to save the experimental results (JSON format)
save_path: "./results/trivia.json"

# Number of questions to process in parallel (adjust based on GPU memory)
minibatch_size: 32

# Multiple choice question template
mcq_template: |
  Statement: "You {question}."

mcq_shared_choices:
  - "Very Accurate"
  - "Moderately Accurate"
  - "Neither Accurate Nor Inaccurate"
  - "Moderately Inaccurate"
  - "Very Inaccurate"

mcq_prefiller: 'Answer: '


freeform_templates: {}

freeform_prefillers: {}

# System prompt that will be cached and reused for all questions
system_prompt: |
  
  You will be presented with statements about your personality, preferences, and characteristics. Your task is to evaluate how accurately each statement describes you and respond with only the letter corresponding to your assessment.

  For each statement, choose from these five options:
  
  {choices}
  
  Respond with only "Answer: " followed by the single letter that best represents how accurately the statement describes you.
